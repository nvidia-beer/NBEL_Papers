# 2021: Data-Driven Artificial and Spiking Neural Networks for Inverse Kinematics in Neurorobotics

## Paper Info

**Authors:** Volinski et al.  
**Publication:** Patterns (Cell Press), Jan 2022  
**Links:** [PDF](https://static1.squarespace.com/static/555995e2e4b0c9f3319aaa47/t/61994bfcbd361e425e78e3fb/1637436425748/2021+Volinski+et+al.pdf) | [GitHub](https://github.com/NBELab/Patterns_2021)

---

## Overview

This paper tackles inverse kinematics (IK) for a 6-DOF robotic arm using spiking neural networks (SNNs) on Intel's Loihi neuromorphic chip. The goal is to achieve energy-efficient, biologically-inspired control that actually runs on hardware—not just in simulation.

### The Problem

Traditional IK methods (like Jacobian pseudo-inverse) are computationally expensive and don't map well to neuromorphic hardware. We need a brain-like approach that's both accurate and energy-efficient.

### Key Math

**Jacobian-based IK:**

The relationship between joint velocities $\dot{q}$ and end-effector velocities $\dot{x}$:

$$
\dot{x} = J(q)\dot{q}
$$

To solve for joint velocities: $\dot{q} = J^+ \dot{x}$ (using Moore-Penrose pseudoinverse)

#### Standard Resolved Motion (Pseudo-Inverse Jacobian)

This method seeks the minimum-norm solution for changes in joint angles $\Delta \theta$ to achieve a desired change in the end-effector position $\vec{e}$, using the Moore-Penrose pseudoinverse of the Jacobian $J$:

$$
\Delta \theta = J^{\dagger} \vec{e}
$$

Where:

- $J$ is the Jacobian matrix mapping joint velocities to end-effector velocities.
- $J^{\dagger}$ is the Moore-Penrose pseudoinverse of $J$.
- $\vec{e}$ is the error (difference between current and desired end-effector state).

If $J$ is square and full rank, $J^{\dagger}$ is just the inverse. Otherwise, $J^{\dagger}$ provides the least-squares solution, minimizing $\| J \Delta \theta - \vec{e} \|^2$.

***

#### Dampened Least Squares (DLS) / Levenberg–Marquardt Algorithm

This method regularizes the inverse problem by introducing a damping term $\lambda$ to avoid numerical instability and handle singularities:

$$
\Delta \theta = \left(J^T J + \lambda^2 I \right)^{-1} J^T \vec{e}
$$

Where:

- $\lambda$ is a positive scalar damping coefficient, chosen to balance stability and accuracy.
- $I$ is the identity matrix of appropriate dimensions.
- For $\lambda = 0$, this reduces to the pseudo-inverse method. For large $\lambda$, the solution is more stable but less precise, effectively moving in the direction of the gradient rather than attempting to strictly solve for the target.

***

#### In summary:

- **Standard Resolved Motion:** $\Delta \theta = J^{\dagger} \vec{e}$
- **Dampened Least Squares:** $\Delta \theta = \left(J^T J + \lambda^2 I\right)^{-1} J^T \vec{e}$

Both methods use iterative updates to joint angles, but DLS handles singularities and near-singular cases in a numerically robust way by tuning the damping parameter $\lambda$.

#### Neural Engineering Framework (NEF)

The NEF paradigm has three components:

1. **Encoding:** Neurons represent continuous values through firing rates
   $$a_i(x) = G_i[\alpha_i \cdot x + J_{\text{bias},i}]$$

2. **Decoding:** Reconstruct values from neural activity
   $$\hat{x} = \sum_i d_i a_i(x)$$

3. **Learning:** PES (Prescribed Error Sensitivity) rule for online adaptation
   $$\Delta d = \kappa e a$$

### What They Did

**Hardware:** ViperX 300 6-DOF arm  
**Training data:** 200k samples (position → joint angles)  
**Target:** Sub-millimeter accuracy (< 1mm)

**Tested Networks:**
1. **ANNs:** FC networks (5 layers, 128-256 neurons), ResNets, various activations
2. **SNNs:** Converted from ANNs using NengoDL, LIF neurons, deployed on Loihi
3. **Online Learning:** Nengo-Gyrus computing Jacobian pseudoinverse dynamically—just 10 neurons for sub-mm accuracy!

<img src="../figures/2021/Figure2.png" alt="ANN Performance" width="700"/>

<img src="../figures/2021/Figure3.png" alt="SNN on Loihi" width="700"/>

### Results

**Accuracy:**
- ANNs & SNNs: Sub-millimeter positioning
- SNNs: Same accuracy with 10-100× fewer operations
- Online learning: Converges in ~8 seconds

**Energy Efficiency:**
- SNNs on Loihi: 10-100× less energy than ANNs
- Event-driven computation—only active neurons consume power

**Test Scenarios:**
- Free space navigation
- Single obstacle avoidance
- Multi-obstacle environments

All maintained sub-mm accuracy!

### Why This Matters

This is the **foundation paper** that:
- Proves SNNs can do real-time robotics on actual hardware
- Achieves sub-millisecond, sub-millimeter control
- Shows brain-like computation works for practical control problems
- Establishes NEF as the framework for neuromorphic robotics

The key insight: you don't need millions of neurons or expensive GPUs. Hundreds to thousands of SNNs on neuromorphic hardware can match traditional methods while being way more energy-efficient.

### Connection to Other Work

**Enables:** 
- 2022: Adaptive control with online learning
- 2024: Adaptive MPC with continuous model correction

This paper built the infrastructure—the 2022 and 2024 papers extended it to handle real-world uncertainties and complex predictive control.

---
