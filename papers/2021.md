# 2021: Data-Driven Artificial and Spiking Neural Networks for Inverse Kinematics in Neurorobotics

## Paper Info

**Authors:** Alex Volinski, Yuval Zaidel, Albert Shalumov, Travis DeWolf, Lazar Supic, Elishai Ezra Tsur  
**Publication:** Patterns (Cell Press), January 14, 2022  
**Links:** [PDF](https://static1.squarespace.com/static/555995e2e4b0c9f3319aaa47/t/61994bfcbd361e425e78e3fb/1637436425748/2021+Volinski+et+al.pdf) | [GitHub](https://github.com/NBELab/Patterns_2021)

## Overview

This paper tackles inverse kinematics (IK) for a simulated 6-DOF robotic arm using both artificial neural networks (ANNs) and spiking neural networks (SNNs) on neuromorphic hardware. The goal is to achieve data-driven, biologically-inspired control that can handle complex, constrained environments.

### The Problem

Traditional IK methods (like Jacobian pseudo-inverse) are computationally expensive and require complete mechanical descriptions and known environments. This work explores data-driven neural network approaches that can handle arbitrary constraints and convoluted environments.

### Key Math

**Jacobian-based IK:**

The relationship between joint velocities $\dot{q}$and end-effector (EE) velocity$\dot{x}$ is:

$$
\dot{x} = J(q) \dot{q}
$$

where $J(q)$ is the Jacobian matrix. For IK, we need the inverse:

$$
\Delta q = J^{\dagger}(q) \Delta x
$$

**Dampened Least Squares (DLS):**

To handle singularities:

$$
\Delta q = J^T (JJ^T + \lambda^2 I)^{-1} \Delta x
$$

where $\lambda$ is a damping factor.

## Methods

### Robot Model

- **Hardware:** Simulated 6-DOF robotic arm (controlling 5 DOF)
- **Dimensions:** Six links of 12, 30, 6, 20, 10, and 20 cm
- **Reach:** 82 cm reach, 1.64 m span
- **Physical design reference:** Based on Zaidel et al., but this work used simulation

### Training Data

- **200,000 uniformly distributed target points** in 2×2×1 m space
- Reachability computed via Jacobian-based numerical optimization
- Split: 70% training, 15% validation, 15% test

### Loss Functions

Three progressively sophisticated loss functions were evaluated:

1. **Naive:** Simple Euclidean distance
   $$
   L = \frac{1}{3}\| x_i - FK(\hat{y}_i) \|_1
   $$

2. **Regularized:** Adds L2 penalty
   $$
   L = \frac{1}{3}\| x_i - FK(\hat{y}_i) \|_1 + \lambda\|\hat{y}_i\|_2^2
   $$
   where $\lambda = 10^{-5}$

3. **Energy-based (Best):** Enforces smooth transitions between neighboring configurations
   $$
   L = \frac{1}{3}\| x'_i - FK(\hat{y}'_i) \|_1 + \frac{1}{3}\| x''_i - FK(\hat{y}''_i) \|_1 + \lambda\|\hat{y}'_i - \hat{y}''_i\|_2^2
   $$
   where $\lambda = 0.1$ and neighboring points are within 3 cm

For obstacle avoidance, an additional penalty term was added:
$$
O = \frac{e^{s(d-r-p_{\text{d}})}}{1 + e^{s(d-r-p_{\text{d}})}}
$$
where $s = 15$(slope),$d$is squared distance to obstacle center,$r$is squared obstacle radius, and$p_{\text{d}} = 25$ (penalty epsilon).

## Architectures Tested

### Artificial Neural Networks (ANNs)

**Fully Connected (FC) Networks:**
- Depths: 2-10 layers
- Widths: 128-256 neurons per layer
- Activations: Tanh, ReLU, Leaky ReLU, Swish, Mish
- **Best performer:** FC 6×256 with energy loss and Mish activation (0.18% above 1 cm accuracy)

**Residual Networks (ResNets):**
- 2-4 blocks with skip connections
- Up to 3.1M parameters
- Comparable performance to FC networks but more parameters

**Training details:**
- Batch SGD with batch size 10
- Initial learning rate: 0.1 (reduced by 20% on plateau)
- Early stopping at 750 epochs

<img src="../figures/2021/Figure2.png" alt="ANN Performance" width="700"/>

### Spiking Neural Networks (SNNs)

> **See [Neural Engineering Framework (NEF)](NEF.md) for detailed mathematical foundations**

Three distinct approaches:

**1. Deep SNNs (ANN-to-SNN Conversion):**
- Converted from ANNs using NengoDL
- Leaky Integrate-and-Fire (LIF) neurons with soft activation:
  $$
  a = \left[\tau_{\text{ref}} - \tau \ln\left(1 - \frac{u_{\text{th}}}{r(I_0 - u_{\text{th}})}\right)\right]^{-1}
  $$
- Requires high firing rates (5,000 Hz) for accuracy
- Synaptic smoothing: 20 ms time constant optimal

**2. SGD-Recurrent SNN:**
- Neuromorphic implementation of gradient descent using Gyrus
- **300,000 parameters**
- Recursively calculates Jacobian and joint corrections
- **Inference time:** 3.8±0.62s on Xavier

**3. Learning-based SNN (Best SNN approach):**
- Uses Prescribed Error Sensitivity (PES) learning rule
- **5,000 neurons** (not 10!)
- Online learning for each target point
- Pre-training option for faster convergence
- **Inference time:** 
  - 2.9±1.22s on Xavier
  - 2.6±0.04s on Loihi (pre-trained)
  - Error convergence at ~2.4s

**PES Learning Rule:**
$$
\Delta d = -\kappa \cdot E \cdot a
$$
where $\kappa$is learning rate,$E$is error, and$a$ is firing rate vector.

> **See [Prescribed Error Sensitivity (PES)](PES.md) for detailed learning rule formulation and stability analysis**

<img src="../figures/2021/Figure3.png" alt="SNN on Loihi" width="700"/>

## Results

### Accuracy

**ANNs (Superior performance):**
- Mean error: **0.2 mm** (with energy loss)
- 1 cm accuracy: >99% of targets
- 1 mm accuracy: High percentage with FC 6×256 Mish
- Energy loss improved mean error by **10× (2 mm → 0.2 mm)**
- Percentage above 1 cm threshold dropped by **10× (60% → 6%)**

**Deep SNNs (Lower accuracy):**
- Mean error: **Few millimeters**
- Performance heavily depends on firing rate
- High firing rates (5,000 Hz) needed for best accuracy
- Generally underperformed ANNs

**Learning-based SNNs:**
- Mean error: **~1 mm for successfully converged points**
- Note: Some points failed to converge, biasing overall mean higher
- Superior to SGD-recurrent approach

### Inference Time

**ANNs (Fast):**
- **0.49-1.34 ms** depending on architecture
- FC 2×128: 0.49±0.15 ms
- FC 6×256: 0.92±0.12 ms
- ResNet 4 blocks: 6.05±0.29 ms
- **Numerical Jacobian (baseline): 16.52±1.26 ms**

**SNNs (Slower but energy-efficient):**
- Deep SNN 4×256: **0.4s** on Loihi
- Learning-based: **2.6-2.9s**
- SGD-recurrent: **3.8±0.62s**

### Energy Efficiency

**Key finding:** SNNs on Loihi consume **10-100× less energy** than ANNs on traditional hardware, despite longer inference times.

**Why?**
- Event-driven computation: only active neurons consume power
- Sparse spiking activity
- Neuromorphic hardware optimized for spike processing

## Geometrically Constrained IK

The networks were tested in environments with obstacles:

### Single Obstacle Performance

**ANNs:**
- Trained with 10 cm and 20 cm spherical obstacles
- **Mish activation: Best performer**
- Obstacle-aware training reduced intersections by **4×**
- Maintained sub-cm accuracy even with constraints

**SNNs (Robust at low firing rates):**
- **Key insight:** Obstacle avoidance performance was similar across firing rates
- Unlike accuracy (which needed high rates), obstacle avoidance worked well at low rates
- Demonstrates energy-efficient robustness
- Synaptic smoothing of 20 ms remained optimal

### Multiple Obstacles (1-5 obstacles)

Tested with randomly positioned 10 cm radius obstacles:

**Results:**
- Mean distance to target: **< 1 cm** across all scenarios
- Obstacle intersections: **< 1** in all tested scenarios (1-5 obstacles)
- Reachable points decreased with more obstacles (as expected)
- Performance depends on obstacle positions, not just quantity

## Test Scenarios

1. **Free space navigation** (200k uniformly distributed points)
2. **Single obstacle avoidance** (10 cm and 20 cm spherical obstacles)
3. **Multi-obstacle environments** (2-5 randomly positioned obstacles)

## Hardware Evaluation

**NVIDIA Xavier (Traditional compute):**
- 512-Core Volta GPU with Tensor Cores
- 8-Core ARM v8.2 64-Bit CPU
- 32 GB RAM
- Used for both ANNs and SNNs

**Intel Loihi (Neuromorphic chip):**
- 128 neuron cores (1,024 neurons each)
- Event-driven spiking computation
- x86 cores for spike routing
- Used for SNNs only
- Deployed using NengoLoihi library (v0.19.14)

## Why This Matters

This is a **foundation paper** that demonstrates:
- SNNs can achieve competitive accuracy with ANNs for robotics control
- Energy efficiency gains of 10-100× on neuromorphic hardware
- Data-driven approaches can handle complex constrained environments
- Both offline (converted SNNs) and online learning approaches are viable

### Key Insights

1. **ANNs remain more accurate** (0.2 mm vs few mm) but require more energy
2. **Energy loss function** was crucial: 10× improvement in accuracy
3. **SNNs excel at energy efficiency** despite longer inference times
4. **Online learning** enables adaptation without extensive pre-training
5. **Obstacle avoidance** works well even at low SNN firing rates

### Connection to Other Work

This work establishes the foundation for:
- **2022 paper:** Adaptive control with online learning and uncertainty handling
- **2024 paper:** Adaptive MPC combining prediction with continuous model correction
- **Future work:** Real-time neuromorphic control on physical robots

The infrastructure and methodology developed here enabled subsequent advances in adaptive and predictive neuromorphic control.
